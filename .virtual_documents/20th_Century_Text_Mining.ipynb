





#importing libraries
from textblob import TextBlob 
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib
import nltk
import re
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
from collections import Counter
sns.set()


myfile = open('Key_events_of_the_20th_century_article_Wiki.txt', encoding='utf-8')


#importing text file 
with open('Key_events_of_the_20th_century_article_Wiki.txt', 'r', errors='ignore') as file:
    data = file.read().replace('\n', '')





# tokenizing words
from nltk.tokenize import word_tokenize
tokenized_word = word_tokenize(data)
print(tokenized_word)


# creating frequency distribution
from nltk.probability import FreqDist
dist_words = FreqDist(tokenized_word)
print(dist_words)





#finding the 10 most common words
dist_words.most_common(10)


#turning list into a dataframe 
top_10_most_common_words = pd.DataFrame(dist_words.most_common(10))


top_10_most_common_words


#changing the column titles
top_10_most_common_words.columns = ['Word', "Occurences"]


#checking the column titles were changed
top_10_most_common_words


#plotting results 
top_10_most_common_words.plot(kind='bar', x='Word', y='Occurences', color='skyblue')
plt.xlabel('Word')
plt.ylabel('Occurences')
plt.title('Key Events of the 20th Century - Top 10 Most Common Words')
plt.legend().set_visible(False)





# Defining stopwords
from nltk.corpus import stopwords
stop_words = set(stopwords.words("english"))
print(stop_words)


# Removing stopwords in words
filtered_words = [] # creates an empty list
for word in tokenized_word:
    if word not in stop_words:
        filtered_words.append(word)


filtered_words


#removing punctuation 

filtered_words_sans_punc = re.sub("[^a-zA-Z]", 
                          " ",          
                          str(filtered_words))


filtered_words_sans_punc


# word tokenization
tokenized_word_2 = word_tokenize(filtered_words_sans_punc)
print(tokenized_word_2)


# creating new frequency distribution
from nltk.probability import FreqDist
dist_words_2 = FreqDist(tokenized_word_2)
print(dist_words_2)


#finding the new 10 most common words
dist_words_2.most_common(10)


new_top_10_most_common_words = pd.DataFrame(dist_words_2.most_common(10))


new_top_10_most_common_words


#changing the column titles
new_top_10_most_common_words.columns = ['Word', "Occurences"]


#checking to see if column titles were changed 
new_top_10_most_common_words


#plotting results 
new_top_10_most_common_words.plot(kind='bar', x='Word', y='Occurences', color='skyblue')
plt.xlabel('Word')
plt.ylabel('Occurences')
plt.title('Key Events of the 20th Century - Top 10 Most Common Words')
plt.legend().set_visible(False)








#filtering more words and letters
new_stopwords = ["And", "Then", 'n', 't', 's', 'The']

filtered = []
for word in tokenized_word_2:
     if word not in new_stopwords:
        filtered.append(word)


%%time
text = TextBlob(str(filtered)) #creating a TextBlob object


text


#creating a tags_list object
tags_list = text.tags


tags_list


df_tags = pd.DataFrame(tags_list) #creating dataframe
df_tags.columns = ['Words', "Word type"] #renaming columns 


df_tags.head()





#grouping words by word type 
df_tags_count = df_tags.groupby('Word type').count().reset_index() 


df_tags_count.head()


#determining the top 10 words types
top_10_word_types = df_tags_count.nlargest(10, 'Words')


top_10_word_types


#plotting top 10 word types
plt.figure(figsize = (10, 5))
with sns.dark_palette("xkcd:blue", 20):
    sns.barplot(x = "Words", y = "Word type",
    saturation = 0.9, data = top_10_word_types).set_title("Key Events of the 20th Century - Top 10 Word Types Used")





#determining the top 10 nouns used
#creating a dataframe with only nouns
nouns_used = df_tags[(df_tags['Word type'] == "NN") | (df_tags['Word type'] == "NNS") | (df_tags['Word type'] == "NNP")]
nouns_used.columns = ["Word", "Occurences"] #renaming column names 
x = nouns_used.groupby('Word').count().reset_index() #grouping words by occurences
y = x.sort_values(by = ['Occurences'], ascending=False) #sorting values 
top_10_nouns = y.nlargest(10, 'Occurences') #identifying the top 10 nouns 


top_10_nouns


#plotting top 10 nouns 
plt.figure(figsize = (10, 5))
with sns.dark_palette("xkcd:blue", 20):
    sns.barplot(x = "Word", y = "Occurences",
    saturation = 0.9, data = top_10_nouns).set_title("Key Events of the 20th Century - Most Frequently Used Nouns")





verbs_used = df_tags[(df_tags['Word type'] == "VB")  | (df_tags['Word type'] == "VBD")]
verbs_used.columns = ["Word", "Occurences"]
x = verbs_used.groupby('Word').count().reset_index()
y = x.sort_values(by = ['Occurences'], ascending=False)
top_10_verbs = y.nlargest(10, 'Occurences')


top_10_verbs


plt.figure(figsize = (10, 5))
with sns.dark_palette("xkcd:blue", 10):
    sns.barplot(x = "Word", y = "Occurences",
    saturation = 0.9, data = top_10_verbs).set_title("Key Events of the 20th Century - Most Frequently Used Verbs")





adjectives_used = df_tags[df_tags['Word type'] == "JJ"]
adjectives_used.columns = ["Word", "Occurences"]
x = adjectives_used.groupby('Word').count().reset_index()
y = x.sort_values(by=['Occurences'], ascending=False)
top_10_adjectives = y.nlargest(10, 'Occurences')


top_10_adjectives


plt.figure(figsize = (10, 5))
with sns.dark_palette("xkcd:blue", 10):
    sns.barplot(x = "Word", y = "Occurences",
    saturation = 0.9, data = top_10_adjectives).set_title("Key Events of the 20th Century - Most Frequently Used Adjectives")











#converting filtered object into a string
listToStr = ' '.join([str(elem) for elem in filtered])

print(listToStr)


# storing results in a dictionary
all_counts = Counter(re.sub(r'\W+', ' ', listToStr).split()) 


all_counts


#importing list of country names 
countries_list = pd.read_csv("countries_list_20th_century.csv", index_col = 0)


countries_list.head()


list_of_countries = countries_list['country_name'].to_list()


dict_of_counts = {d : all_counts[d] for d in list_of_countries}


dict_of_counts


#making sure list_of_countries and all_counts formats match
print("list_of_countries:")
print(list_of_countries)  


print("all_counts keys:")
print(all_counts.keys()) 


#in list_of_countries, there are spaces before and after the country_name
#taking away the spaces to make it match the format of all_counts 
normalized_all_counts = {key.strip(): value for key, value in all_counts.items()}
normalized_countries_list = [country_name.strip() for country_name in list_of_countries]


print("Normalized all_counts:", normalized_all_counts)


#checking to see the spaces have been removed
print("Normalized countries_list:", normalized_countries_list)


dict_of_counts = {country_name: normalized_all_counts.get(country_name, 0) for country_name in normalized_countries_list}


#checking to see whether it worked 
dict_of_counts


dct = {v:[k] for v,k in dict_of_counts.items()}  
country_mentions = pd.DataFrame(dct)


country_mentions


country_mentions = country_mentions.transpose().reset_index()


country_mentions


country_mentions.rename(columns = {"index":"Country Name", 0:"Times mentioned"}, inplace = True)


country_mentions


import os


#creating path
path = r'C:\Users\canam\OneDrive\Desktop\Career Foundry'


# saving the dataframe as csv
country_mentions.to_csv(os.path.join(path, 'Specialization', '20th-Century', 'country_mentions.csv'))


#sorting the number of country mentions 
sorted_country_mentions = country_mentions.sort_values(by='Times mentioned', ascending=False)


sorted_country_mentions.head(20)


#identifying the top 10 mentioned countries 
top_20_mentioned_countries = sorted_country_mentions.head(20)


#plotting top_20_mentioned_countries 
plt.figure(figsize = (10, 5))
with sns.dark_palette("xkcd:blue", 10):
    sns.barplot(x = "Times mentioned", y = "Country Name",
    saturation = 0.9, data = top_20_mentioned_countries).set_title("Key Events of the 20th Century - Most Frequently Mentioned Countries")

















text_sent = TextBlob(str(filtered))


print(text_sent.sentiment)



